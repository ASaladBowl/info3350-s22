{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 12: Classification II\n",
    "\n",
    "## To do\n",
    "\n",
    "* HW 6 (corpus building) due Thursday, 11:59pm\n",
    "    * Make sure you have the updated guidelines from CMS (updated Saturday afternoon, March 5)\n",
    "* This week\n",
    "    * Today: Read HDA, ch. 8 and \"The curse(s) of dimensionality\"\n",
    "    * Wednesday: Read articles by Underwood and by Norvig\n",
    "        * Response to Underwood or Norvig due by 4:00pm by Tuesday, 3/8, if assigned to you by NetID.\n",
    "    * Friday: Section as usual\n",
    "* Next week (feature importance and explainability)\n",
    "    * Monday: Read \"The importance of human-interpretable ML\"\n",
    "    * Wednesday: Read articles by Underwood and by Yauney\n",
    "        * Response to Underwood or Yauney due by 4:00pm by next Tuesday, 3/15, if assigned to you by NetID.\n",
    "    * Friday: Section as usual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "### *k* Nearest Neighbors\n",
    "* Assign to new objects the most common label among *k* nearest neighbors in the training set\n",
    "* Very low computational cost, easy to understand and interpret, and can fit complex decision spaces with ease\n",
    "* Not usually the best performance for real-world cases\n",
    "\n",
    "### Decision tree\n",
    "* Select features that best split the training data, in descending order\n",
    "* Need a measure of the empirical quality of the split imposed by a given partition \n",
    "    * We are trying to reduce entropy, that is, the mixing of the classes that would follow from splitting at a given value of a feature\n",
    "    * Mutual information (recall HDA, ch. 5) is a typical metric to select the most informative features\n",
    "* Highly interpretable\n",
    "* Prone to **overfitting**\n",
    "\n",
    "<img src=\"./images/Decision_Tree.jpg\">\n",
    "\n",
    "### Random forest\n",
    "* Create many different decision trees trained on different subsets of your data\n",
    "* Class label by majority vote of the different trees\n",
    "* Improves overfitting of single decision trees, at the cost of interpretability\n",
    "\n",
    "### Logistic regression\n",
    "* Begin with the intuition that the value of each feature measured for each object pushes us (and our classifier) toward the belief that the object belongs to one category or another.\n",
    "    * Features may be in conflict, in the sense that an object may have some features that are associated with one class and others that are associated with another class.\n",
    "    * We want to assign an object to the category that best aligns with its features overall.\n",
    "* Fit training data to a linear model: $z = W_0 + W_1 x_1 + W_2 x_2 + ...$\n",
    "    * Note that values of $z$ can range from -infinity to + infinity\n",
    "    * This linear function summarizes our evidence\n",
    "    * More positive values suggest membership in one class; more negative values suggest membership in the other class\n",
    "* Transform the linear value into a score between 0 and 1 using the sigmoid function: $$\\sigma(t) = \\frac{1}{1 + e^{-z}}$$\n",
    "    * This alows us to transform the infinite range of evidentiary values from the linear model into a value that ranges from zero to one and that we can interpret as a probability of membership in the target class.\n",
    "* Sigmoid function looks like this:\n",
    "    \n",
    "<img src=\"./images/sigmoid.png\">\n",
    "\n",
    "* Logistic regression:\n",
    "    * Is pretty interpretable\n",
    "    * Is relatively lightweight\n",
    "    * Often performs acceptably\n",
    "    * Is the main basis for neural networks, where each neuron is in effect a logit classifier\n",
    "\n",
    "### Na√Øve Bayes\n",
    "* Covered in HDA, ch. 6\n",
    "* Bayes' Rule: $p(C|x_1 , ..., x_n) \\propto p(C)\\prod_{i=1}^{n} p(x_i | C)$\n",
    "* Read: \"The probability of class $C$ given the values in vector $x$ equals the underlying probability of class $C$ times the probabilities of those feature values given class $C$.\"\n",
    "* This allows you to transform a difficult (or impossible) measurement, the probability of class membership given observed evidence, into an easy (or easier) one that involves the probability of seeing certain evidence given class membership.\n",
    "* $x_i$ is the value of feature $i$ in the vector $x$\n",
    "* The latter part -- the underlying proabability of class $C$ and the probability of the feature values given class $C$ -- is what you learn from your training data.\n",
    "* You can then assign class labels in your test set on the basis of the class with the highest predicted probability given the evidence.\n",
    "* Note difference between Bernoulli, Gaussian, and multinomial NB (each of which is a different classifier type in `sklearn`): \n",
    "    * Bernoulli NB is for use with binary features (encoding the presence or absence of a word type in a document, for example). You would use this if you binarized your features at vectorization.\n",
    "    * Multinomial NB is for features that take integer values, such as ordinary word count.\n",
    "    * Gaussian NB is for use with continuous variables, including TF-IDF weighted and/or normalized word counts.\n",
    "    * `sklearn` will often take care of reformatting your data for use with the NB classifier you select, but it may not result in the treatment you intended.\n",
    "\n",
    "### SVM\n",
    "* A linear (by default) method based on finding the line or plane that separates classes with maximum margin.\n",
    "    * Think of this as a line of \"worst fit,\" in contrast to the line of best fit for a regression model.\n",
    "    * The *support vectors* are the objects closest to the decision boundary, i.e., those that would *change* the boundary if they were removed.\n",
    "* Can be extended to nonlinear hyperplanes via kernel functions.\n",
    "    * A kernel function maps input points to a higher-dimensional space in which the inputs *are* linearly separable.\n",
    "* A classic approach to text classification (developed in the 1990s, in part by Thorsten Joachims here at Cornell; underpinnings extend back to the 1960s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## An artificial example\n",
    "\n",
    "Let's try a few different classifiers in `sklearn`.\n",
    "\n",
    "**NB.** We use the `make_blobs` function to create synthetic data for purposes of illustration. This has no role in actual clustering or classification problems. Don't try to use `make_blobs` as part of your approach to learning from real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few sklearn classifiers on synthetic data\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# data parameters\n",
    "n_samples = 1500\n",
    "n_blobs=4\n",
    "\n",
    "# make data with different variance\n",
    "X_varied, y_varied = make_blobs(\n",
    "    n_samples=n_samples, \n",
    "    centers=n_blobs,\n",
    "    cluster_std=np.random.choice( # randomize variance of each cluster\n",
    "        [1.0, 5.0, 2.0], \n",
    "        size=n_blobs, \n",
    "        replace=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# knn\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5) # instantiate\n",
    "knn_clf.fit(X_varied, y_varied)               # fit\n",
    "y_knn = knn_clf.predict(X_varied)             # predict\n",
    "\n",
    "# logistic regression\n",
    "logit_clf = LogisticRegression()\n",
    "logit_clf.fit(X_varied, y_varied)\n",
    "y_logit = logit_clf.predict(X_varied)\n",
    "\n",
    "# svm\n",
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_varied, y_varied)\n",
    "y_svm = svm_clf.predict(X_varied)\n",
    "\n",
    "# plot results\n",
    "# set up plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "# input data\n",
    "plt.subplot(141)\n",
    "plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_varied, alpha=0.5)\n",
    "plt.title(\"Ground truth\")\n",
    "# knn\n",
    "plt.subplot(142)\n",
    "plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_knn, alpha=0.5)\n",
    "plt.title(\"kNN\")\n",
    "# logit\n",
    "plt.subplot(143)\n",
    "plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_logit, alpha=0.5)\n",
    "plt.title(\"Logit\")\n",
    "# svm\n",
    "plt.subplot(144)\n",
    "plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_svm, alpha=0.5)\n",
    "plt.title(\"SVM\")\n",
    "plt.show()\n",
    "\n",
    "# scoring (naive, not cross-validated)\n",
    "print(f\"kNN score:   {knn_clf.score(X_varied, y_varied):.3f}\")\n",
    "print(f\"Logit score: {logit_clf.score(X_varied, y_varied):.3f}\")\n",
    "print(f\"SVM score:   {svm_clf.score(X_varied, y_varied):.3f}\")\n",
    "\n",
    "# scoring (with cross validation)\n",
    "print(f\"\\nkNN score, cross validated:   {np.mean(cross_val_score(knn_clf, X_varied, y_varied)):.3f}\")\n",
    "print(f\"Logit score, cross validated: {np.mean(cross_val_score(logit_clf, X_varied, y_varied)):.3f}\")\n",
    "print(f\"SVM score, cross validated:   {np.mean(cross_val_score(svm_clf, X_varied, y_varied)):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the modest difference in accuracy that comes from cross validation (not fitting and testing on the same data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
