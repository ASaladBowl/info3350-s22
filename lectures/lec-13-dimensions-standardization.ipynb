{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 13: Feature standardization, normalization, and dimension reduction\n",
    "\n",
    "## To do\n",
    "\n",
    "* HW 6 (corpus building) due Thursday, 11:59pm\n",
    "    * Make sure you have the updated guidelines from CMS (updated Saturday afternoon, March 5)\n",
    "* Next week (feature importance and explainability)\n",
    "    * Monday: Read \"The importance of human-interpretable ML\"\n",
    "    * Wednesday: Read articles by Underwood and by Yauney\n",
    "        * Response to Underwood or Yauney due by 4:00pm by next Tuesday, 3/15, if assigned to you by NetID.\n",
    "    * Friday: Section as usual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring classifier performance\n",
    "\n",
    "In this lecture -- and for the next couple of weeks -- we'll talk about factors that affect classifier performance. But how should we *meausre* performance? \n",
    "\n",
    "In the last lecture, we scored our classifiers on **accuracy**, that is, the fraction of all predictions that are correct. This can be fine, but it's not well suited to cases in which we have different-size classes and it doesn't capture whether we do equally well on any one class.\n",
    "\n",
    "Machine learning tends instead to measure performance using **precision**, **recall**, and $F_1$. Here's what those terms mean:\n",
    "\n",
    "* **Precision** is the fraction of positive predictions for a given class that are correct. In other words, when we predict that an object belongs to a class, how likely are we to be right about it?\n",
    "* **Recall** is the fraction of all positive instances in the data set that are predicted positive. In other words, from among all the objects we *should* have assigned to a class, what fraction of them did we catch?\n",
    "\n",
    "You can see that precision and recall tend to involve a tradeoff in real-world settings. You can achieve high precision if you're very conservative about labeling marginal cases. You can get high recall by being much more liberal with your labels. Consider two limit cases of a simple problem:\n",
    "\n",
    "> I ask you to bring me all the books by Toni Morrison that are held in the Cornell libraries. Favoring precision, you bring me just one book, a first-edition copy of _The Bluest Eye_ (great book, by the way) and nothing else. You're right that it's by Morrison, so your precision is perfect (1.0). But your recall is really bad. If there are 108 Morrison volumes in the library (which there are, for certain deifnitions of \"by\", \"in\", \"Cornell,\" \"libraries,\" and \"books\"), you have correctly retrieved less than 1% of them. Or maybe you favor recall, so you bring me every book in the library (about 8 million of them). Yikes. In this case, recall is perfect (you *did* bring me every book by Morrison), but your precision is terrible (of the 8 million books you labeled as \"by Toni Morrison,\" only 20 of them were correct (about 0.00025%).\n",
    "\n",
    "* $F_1$ is one way to assess the combination of precision and recall. It's just the *harmonic mean* of those two numbers. A harmonic mean is a conservative estimate of the average value, because it is affected more by small values than by large ones. \n",
    "    * You don't really need to know how to calculate $F_1$, but it's not hard. You take the reciprocal of the (familiar, plain vanilla) arithmetic mean of the reciprocals of your observations.\n",
    "    * In the first Morrison example (high precision, low recall), precision = 1.0 and recall = 0.01 (rounding a bit). \n",
    "    * The regular (non-harmonic) mean would be about 0.5 ($=(1.0 + 0.01)/2$)\n",
    "    * $F_1$ is:\n",
    "    \n",
    "$$F_1 = \\left( \\frac{1.0^{-1}+0.01^{-1}}{2} \\right)^{-1} = \\frac{2}{1 + 100} = 0.02$$\n",
    "\n",
    "$F_1$ for the high-recall case would be even worse, since precision in that case is even closer to zero. Most real-world cases aren't as extreme as these, but the idea is the same. $F_1$ avoids artificially inflated estimates of classifier performance. In general, it's what you (and other machine learning researchers) want to use as your accuracy/performance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Normalization and standardization\n",
    "\n",
    "First, note that the terms \"normalization\" and \"standardization\" are sometimes used interchangably or inconsistently. So be sure you understand what's going on in any given instance if you encounter there terms in the wild. In this class, we'll try to be consistent with the definitions below.\n",
    "\n",
    "### Normalization (`l1`, `l2`)\n",
    "\n",
    "* Normalization is the process by which we ensure that each vector in our feature matrix has the same length\n",
    "    * Normalization works *row-wise*, not *column-wise*. That is, we normalize the features attached to each *observation*.\n",
    "* The details vary, depending on our distance (length) metric\n",
    "* The two most commonly used normalizations are `l1` and `l2`\n",
    "* `l1` normalization adjusts the features so that they sum to one in each vector.\n",
    "    * This means that the *Manhattan length* of each vector is one\n",
    "    * It also means that the normalized value of each feature represents the fraction of the total feature counts/weight accounted for by that feature in a given document\n",
    "    * As previously discussed, `l1` norms preserve the original ratios between the features. This is often good for accuracy on downstream tasks.\n",
    "* `l2` normalization adjusts each vector so that the sum of the squared features is one\n",
    "    * This means that the *Euclidean length* of each vector is one\n",
    "    * `l2` norms decrease the effective weight of low-frequency features (hence, they increase the relative weight of high-frequency features). This can be good for interpretation, because it means that downstream tasks rely on a comparatively sparse set of important features.\n",
    "\n",
    "\n",
    "### Standardization (*z*-scores)\n",
    "\n",
    "* Even when we normalize our vectors, distances and similarities are dominated by common terms\n",
    "    * Common terms contribute most of the weight to the overall vector\n",
    "    * This might be what we want ...\n",
    "    * ... or it might not.\n",
    "* What if we care about the comparative usage rates of each included feature (word)?\n",
    "    * That is, what if every word should contribute equally?\n",
    "    * We could scale between, say, 0 and 1\n",
    "        * But then we're at the mercy of high and low outliers\n",
    "    * Instead, we often scale to mean zero and standard deviation one.\n",
    "        * This is called a \"standard score\" or \"*z*-score.\"\n",
    "* Standardization works *column-wise*. That is, it makes every *feature*, across all observations, as important as every other feature.\n",
    "\n",
    "Typically, you'll normalize first, then scale. But some situations call for just one or the other, or are not especially sensitive to either one. You might *not* want to normalize or scale if your feature data share a common (and meaningful) scale and are already normally distributed. Leaving your data alone in that case may not make much diffrence for your task performance and may make it easier to interpret your results.\n",
    "\n",
    "# An example\n",
    "\n",
    "Consider the following example of normalization and standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from   sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize\n",
    "from   sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "sample = [\n",
    "    [10,11,11,12],\n",
    "    [0,0,1,1],\n",
    "    [0,5,7,10]\n",
    "]\n",
    "df = pd.DataFrame(sample).T\n",
    "df.columns = ['the', 'cat', 'she']\n",
    "\n",
    "print('raw features:')\n",
    "display(df)\n",
    "print(\"raw mean:\", round(np.mean(df.to_numpy()),2))\n",
    "print('raw distances:')\n",
    "display(pd.DataFrame(euclidean_distances(df)).mask(lambda x: x==0,np.nan).style.background_gradient(cmap='RdYlGn', axis=None))\n",
    "\n",
    "print('\\n\\nmin-max scaling:')\n",
    "minmax = MinMaxScaler().fit_transform(df)\n",
    "display(minmax)\n",
    "print(\"min-max mean:\", round(np.mean(minmax),3))\n",
    "print('min-max distances:')\n",
    "display(pd.DataFrame(euclidean_distances(minmax)).mask(lambda x: x==0,np.nan).style.background_gradient(cmap='RdYlGn', axis=None))\n",
    "\n",
    "print('\\n\\nz-scores:')\n",
    "zscores = StandardScaler().fit_transform(df)\n",
    "display(zscores)\n",
    "print('z-score mean:', round(np.mean(zscores),3))\n",
    "print('z-score distances:')\n",
    "display(pd.DataFrame(euclidean_distances(zscores)).mask(lambda x: x==0,np.nan).style.background_gradient(cmap='RdYlGn', axis=None))\n",
    "\n",
    "print('\\n=====\\nl1 norm:')\n",
    "l1 = normalize(df, norm='l1')\n",
    "display(l1)\n",
    "print('l1 mean:', round(np.mean(l1),3))\n",
    "print('l1 distances:')\n",
    "display(pd.DataFrame(euclidean_distances(l1)).mask(lambda x: x==0,np.nan).style.background_gradient(cmap='RdYlGn', axis=None))\n",
    "\n",
    "print('\\n\\nl2 norm:')\n",
    "l2 = normalize(df, norm='l2')\n",
    "display(l2)\n",
    "print('l2 mean:', round(np.mean(l2),3))\n",
    "print('l2 distances:')\n",
    "display(pd.DataFrame(euclidean_distances(l2)).mask(lambda x: x==0,np.nan).style.background_gradient(cmap='RdYlGn', axis=None))\n",
    "\n",
    "print('\\n=====\\nz-scored l2 norm:')\n",
    "zl2 = StandardScaler().fit_transform(l2)\n",
    "display(zl2)\n",
    "print('z-scored l2 mean:', round(np.mean(zl2),3))\n",
    "print('z-scored l2 distances:')\n",
    "display(pd.DataFrame(euclidean_distances(zl2)).mask(lambda x: x==0,np.nan).style.background_gradient(cmap='RdYlGn', axis=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a single plot with all six feature (not distance) matrices\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,8))\n",
    "for i, mat in enumerate([df, minmax, zscores, l1, l2, zl2]):\n",
    "    plt.subplot(int(f'23{i+1}'))\n",
    "    plt.imshow(mat)\n",
    "    plt.title(i)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**You almost certainly want to standardize your feature data, probably with *z*-scores, unless there's a good reason to do otherwise.**\n",
    "\n",
    "## Dimension reduction\n",
    "\n",
    "### The curse of dimensionality\n",
    "\n",
    "* Low density of samples\n",
    "    * Hard to identify \"typical\" or \"average\" points. \n",
    "    * Everything is an outlier.\n",
    "    * All points are far apart (or have low similarity, or are uncorrelated).\n",
    "* Multicolinearity\n",
    "    * Always true when you have more dimensions than samples.\n",
    "    * Many variables might be substituted for one another.\n",
    "    * But *which ones*?\n",
    "    * This is a problem if we want to *interpret* our model.\n",
    "* Overfitting\n",
    "    * Too much \"detail\" in our training data.\n",
    "    * For example, say we care about cats in our texts.\n",
    "        * Do we need features `['cat', 'cats', 'kitten', 'kittens', 'kitty', 'kitties', 'Cat', 'Cats', ...]`?\n",
    "        * Probably not; any one of these, or their sum, would do.\n",
    "\n",
    "The trick, then, is to figure out *which* features to keep (feature selection) and/or how collapse multiple features into one (dimension reduction).\n",
    "\n",
    "Note that this explains why the Decision Tree classifier with a shallow tree depth performed well in the current problem set: it effectively performs dimension reduction via feature selection.\n",
    "\n",
    "## Feature selection\n",
    "\n",
    "We'll talk more about this later, but for now, a few key points:\n",
    "\n",
    "* If a feature has the same value for most objects (that is, it has low variance), it is unlikely to be informative and is a good candidate for elimination.\n",
    "* We're looking to hold on to as much of the underlying variance (information) in the data as possible, while eliminating as many features as possible.\n",
    "    * Any measure of correlation or mutual information would help us identify features that provide similar information.\n",
    "    * We might then drop one or more of those variables with little loss of overall information.\n",
    "* We can also work empirically and *post hoc* by calculating feature importances from our classifier (where possible).\n",
    "    * We then retain only the *n* most important features and examine the impact on classifier performance.\n",
    "\n",
    "\n",
    "## Linear and manifold methods\n",
    "\n",
    "But we can also *transform* our features, rather than just dropping some and retaining others.\n",
    "\n",
    "Specifically, we can look for mathematical *combinations* of features that hold on to all or most of the underlying variance.\n",
    "\n",
    "Consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linearly related variables\n",
    "x = np.linspace(0,5)\n",
    "y = x\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we know *x*, we already know the *exact* value of *y*!\n",
    "* Here, we could just drop *x* or *y*.\n",
    "\n",
    "But what about this case?:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=1000, centers=1)\n",
    "transformation = [[-0.6, -0.6], [0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "plt.scatter(X_aniso[:,0], X_aniso[:,1], alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# A function to draw vectors\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# Set up and fit PCA\n",
    "pca = PCA(n_components=2, whiten=True) # whiten enforces some extra regularity on output\n",
    "X_pca = pca.fit_transform(X_aniso)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(3,1, figsize=(4.5, 12))\n",
    "\n",
    "# Input data\n",
    "ax[0].scatter(X_aniso[:, 0], X_aniso[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v, ax=ax[0])\n",
    "ax[0].axis('equal');\n",
    "ax[0].set(xlabel='x', ylabel='y', title='Input')\n",
    "\n",
    "# PCA 2-D\n",
    "ax[1].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.2)\n",
    "draw_vector([0, 0], [0, 3], ax=ax[1])\n",
    "draw_vector([0, 0], [3, 0], ax=ax[1])\n",
    "ax[1].axis('equal')\n",
    "ax[1].set(xlabel='PC0', ylabel='PC1',\n",
    "          title='PCs')\n",
    "\n",
    "# PCA 1-D\n",
    "ax[2].scatter(X_pca[:,0], np.zeros(shape=len(X_aniso)), alpha=0.1)\n",
    "ax[2].set(xlabel='PC0', ylabel='None', title='1-D')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Explained variance:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated SVD\n",
    "\n",
    "Singular Value Decomposition (SVD) is closely related to PCA. The only difference, from our perspective, is that, because PCA needs to standardize input data, it requires dense (rather than sparse) input. So, we use SVD (via `TrunctedSVD`) when we want to preserve input sparsity (e.g., when our dataset is very large). \"Truncated\" just means that we retain fewer dimensions in our output than existed in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=1)\n",
    "X = svd.fit_transform(X_aniso)\n",
    "plt.scatter(X[:,0], np.zeros(shape=len(X)), alpha=0.1)\n",
    "plt.show()\n",
    "print('Explained variance:', round(svd.explained_variance_ratio_[0],4))\n",
    "svd.fit_transform(StandardScaler().fit_transform(X_aniso))\n",
    "print('Explained variance using standardized data:', round(svd.explained_variance_ratio_[0],4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* Standardizing our input data captures about the same amount of variance as does PCA. It's slightly different because we used the `whiten` option with PCA, which assures unit variance in each each reduced output dimension at the typical cost of a small amount of lost variance.\n",
    "* We've used a one-dimensional plot here *not* because SVD is doing something different from PCA, but to show what dimension *reduction* looks like. We had 2-D inputs; it doesn't really make sense to use 2-D outputs!\n",
    "\n",
    "### *t*-SNE\n",
    "\n",
    "*t*-distributed Stochastic Neighbor Embedding is a *manifold* method. Features are projected into a multidimensional manifold rather than onto lines.\n",
    "\n",
    "TSNE is (or was; see below) widely used for visualization, because it's good at maintaining internal structure (\"lumpiness\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(init='random', learning_rate='auto')\n",
    "X = tsne.fit_transform(X_aniso)\n",
    "plt.scatter(X[:,0], X[:,1], alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP\n",
    "\n",
    "Uniform Manifold Approximation and Projection (UMAP) is a manifold method, like *t*-SNE. It's more computationally efficient than *t*-SNE and tends to perform a bit better, too (in the sense that it preserves more of the underlying density structure. UMAP is generally preferred to *t*-SNE for visualization these days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "umap_reducer = umap.UMAP()\n",
    "X = umap_reducer.fit_transform(X_aniso)\n",
    "plt.scatter(X[:,0], X[:,1], alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a bunch of parameters that control the performance of UMAP (and of *t*-SNE, too). If you make any real use of UMAP, you should read (and understand) [the documentation](https://umap-learn.readthedocs.io/en/latest/parameters.html#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
