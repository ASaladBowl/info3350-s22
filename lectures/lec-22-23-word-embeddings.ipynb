{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 22: Static word embeddings\n",
    "\n",
    "## To do\n",
    "\n",
    "* Monday (today): Ruder\n",
    "* Wednesday: Nelson (on Canvas)\n",
    "    * Respond by tomorrow (Tuesday) afternoon if it's your week\n",
    "* HW7 due Thursday night\n",
    "* Section meetings on Friday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words as vectors\n",
    "\n",
    "**Big picture: static word embeddings replace each word in our corpus with a vector** (typically of dimensionality between 100 and 1,000) **that captures the range of meanings of that word. We can use these vectors in place of token counts as the input features for any downstream NLP or text analysis task.**\n",
    "\n",
    "* We've previously represented **documents** as vectors\n",
    "    * Usually, vectors of word counts\n",
    "    * Or, with topic models, as vectors of topics\n",
    "* Limits of words, often discussed\n",
    "    * In brief: some words are more alike than others, but word counts don't reflect this fact\n",
    "    * Topic models helped to address this problem\n",
    "    * Today, a different approach: **word embeddings**\n",
    "* We might like to have a vector for each **word**, which we could in turn use to build our document vectors\n",
    "    * This will be an *unsupervised* task. We want to produce vectors that place similar words close together in vector space, but we don't want to use human-labled data to do so. We want instead to observe patterns of use in a large corpus.\n",
    "* One approach (not pursued here): Truncated SVD on sliding windows over words\n",
    "    * Group all the resulting bags of words by their 'center' word\n",
    "    * Reduce resulting \"doc\"-term matrix for each word via truncated SVD\n",
    "        * Like latent semantic analysis, but oriented around words rather than documents\n",
    "    * Relatively fast to train and decent performance\n",
    "    * Limited by linear relationships and lack of semantic connections\n",
    "* Language models\n",
    "    * Large neural language models achieve state-of-the-art performance on many (all?) NLP tasks\n",
    "    * These models generally seek to predict a word (or other element of a text sequence) on the basis of the words around it\n",
    "    * General architecture is:\n",
    "        * Embedding layer. Counts of words transformed into contextual vectors. Initially, weights are random. In the case of word embeddings, we want to learn the proper weights as our output objective.\n",
    "        * Intermediate (hidden) layer. Neural layers that can learn non-linear relationships between words and embeddings.\n",
    "        * Output layer. Optimize an objective function (for example, highest probability of next word, given an input sequence).\n",
    "    * `word2vec` (2013) showed that we can do without the intermediate layer if we just want to produce word embeddings\n",
    "        * This simplifies the computation, so we can use *much* more input text\n",
    "* CBOW vs. skip-gram\n",
    "    * `word2vec` can use two different models: Continuous bag of words (CBOW) and skip-gram\n",
    "    * CBOW is more \"traditional.\" Objective is to predict a word given the *n* words before and after it.\n",
    "    * Skip-gram \"flips\" the prediction task. Try to predict the *n* contextual words, given a seed word.\n",
    "    * CBOW is faster to train and better captures *syntax* (plurals are closer to other plurals in vector space)\n",
    "    * Skip-gram is slower to train and better captures *semantic* relationships ('king' moves  closer to 'prince' than to 'kings')\n",
    "* Training is slow either way, so using pretrained embeddings is standard practice, but ...\n",
    "    * Embedded relationships and biases of training set\n",
    "    * Change over time\n",
    "    * Polysemy remains within the embedding\n",
    "        * \"Class\" vector has components of education and of social status, for example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's *use* some embeddings\n",
    "\n",
    "We're not going to train them, because that's slow and requires lots of corpus text. If we have time, we'll at least mention transfer learning in a future lecture. Transfer learning allows us to use a modest amount of new training data to update a larger, pretrained model.\n",
    "\n",
    "### We need a `spaCy` medium or large model\n",
    "\n",
    "The small models that we used in the previous lecture do not include vectors. Run the code below in a terminal to download and install the large model, which includes `GloVe` embeddings (`GloVe` is an alternative to `word2vec`).\n",
    "\n",
    "```\n",
    "python -m spacy download en_core_web_lg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\") # Note '_lg' = large model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king -> queen similarity: 0.7252610325813293\n"
     ]
    }
   ],
   "source": [
    "# How similar are the words 'king' and 'queen' in Web text?\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"king -> queen similarity:\", cosine_similarity(\n",
    "    nlp.vocab['king'].vector.reshape(1,-1), # reshape into a 2d vector with single row\n",
    "    nlp.vocab['queen'].vector.reshape(1,-1)\n",
    ").item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector math\n",
    "\n",
    "Now we ask: can we remove some part of a word's meaning, add a different part, and end up closer to a specific endpoint?\n",
    "\n",
    "Another way to thing about this is to imagine the task as an analogy. For example, `man:king :: woman: ___`. Many readers would complete the analogy with `queen`. Let's see if `word2vec` does something similar ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man + woman -> queen similarity: 0.7880844473838806\n"
     ]
    }
   ],
   "source": [
    "# Vector math: king - man + woman -> closer to queen?\n",
    "king_to_queen_vector = nlp.vocab['king'].vector - nlp.vocab['man'].vector + nlp.vocab['woman'].vector\n",
    "print(\"king - man + woman -> queen similarity:\", cosine_similarity(\n",
    "    king_to_queen_vector.reshape(1,-1), \n",
    "    nlp.vocab['queen'].vector.reshape(1,-1)\n",
    ").item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we've moved closer to `queen` by subtracting `man` and adding `woman`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris -> Berlin:                    0.5989030599594116\n",
      "Paris - France + Germany -> Berlin: 0.7547403573989868\n"
     ]
    }
   ],
   "source": [
    "# Try the same with Paris and Berlin\n",
    "print(\"Paris -> Berlin:                   \", cosine_similarity(\n",
    "    nlp.vocab['Paris'].vector.reshape(1,-1), # reshape into a 2d vector with single row\n",
    "    nlp.vocab['Berlin'].vector.reshape(1,-1)\n",
    ").item())\n",
    "\n",
    "paris_to_berlin_vector = nlp.vocab['Paris'].vector - nlp.vocab['France'].vector + nlp.vocab['Germany'].vector\n",
    "print(\"Paris - France + Germany -> Berlin:\", cosine_similarity(\n",
    "    paris_to_berlin_vector.reshape(1,-1), \n",
    "    nlp.vocab['Berlin'].vector.reshape(1,-1)\n",
    ").item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can use vectors on these kinds of \"analogy\" tasks. How could we find the *n* most similar words to a given vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus-wide similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 708206\n",
      "Has vectors: 684752\n",
      "CPU times: user 14.7 s, sys: 96.9 ms, total: 14.8 s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# How large is our model?\n",
    "string_count = 0\n",
    "vector_count = 0\n",
    "for string in nlp.vocab.strings:\n",
    "    string_count+=1\n",
    "    if nlp.vocab[string].has_vector:\n",
    "        vector_count+=1\n",
    "print(\"Vocab size:\", string_count)\n",
    "print(\"Has vectors:\", vector_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our language model has 684,752 words with embeddings (and about 25,000 more words for which we lack embeddings; we'll ignore those).\n",
    "\n",
    "Next, we'll create a word-embedding matrix over our vocabulary, so that we can calculate distances between any pair of words. This matrix will have 684,752 rows (one for each word that has an embedding in our model) and 300 columns (the number of dimensions in the embedding representation).\n",
    "\n",
    "We initialize the output matrix with zeros, since it's much faster to update an existing numpy matrix than it is to append to it.\n",
    "\n",
    "Note that there's an easier way to find the most similar vectors to a taget word in spaCy, but we're showing the details for clarity ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (684752, 300)\n",
      "CPU times: user 14.6 s, sys: 764 ms, total: 15.3 s\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Make a word-vector martrix with labels\n",
    "import numpy as np\n",
    "vector_matrix = np.zeros([vector_count,nlp.vocab.vectors_length]) # Initialize the output matrix\n",
    "counter = 0\n",
    "vocab_dict = {} # Dictionary to hold word index positions in the matrix\n",
    "vocab_list = [] # List to hold words in order\n",
    "for string in nlp.vocab.strings: # iterate over the strings in our model\n",
    "    if nlp.vocab[string].has_vector: # only want the ones with embeddings\n",
    "        vocab_dict[string] = counter # record position of this word\n",
    "        vocab_list.append(string) # add to our list of words\n",
    "        # l2-normalize the vector and update matrix\n",
    "        vector_matrix[counter] = nlp.vocab[string].vector/nlp.vocab[string].vector_norm\n",
    "        counter+=1 # increment counter\n",
    "print(\"Matrix shape:\", vector_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be tempted to iterate over the rows in the output matrix, calculating the similarity between each one and your target vector. This is slow. Instead, we'll use matrix math.\n",
    "\n",
    "The **dot product** of two 1D vectors is the sum of their element-wise products. If the vectors are (L2) normalized, their dot product = their cosine similarity. If you take the dot product of a 2D matrix with a 1D vector, the result is a 1D vector representing the dot products of the 1D vector with each of the row vectors in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of vec1.vec2: 0.51\n"
     ]
    }
   ],
   "source": [
    "# example dot product\n",
    "vec1 = [0.5, 0.1]\n",
    "vec2 = [1.0, 0.1]\n",
    "result = 0.5*1.0 + 0.1*0.1\n",
    "print(\"Result of vec1.vec2:\", result)\n",
    "assert np.dot(vec1, vec2) == result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(684752,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate similarities to 'king' via dot products\n",
    "similarities = np.dot(vector_matrix, vector_matrix[vocab_dict['king']])\n",
    "similarities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that NumPy dot products are really fast to calculate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KIng 1.0000000265848845\n",
      "King 1.0000000265848845\n",
      "KING 1.0000000265848845\n",
      "king 1.0000000265848845\n",
      "Kings 0.7876614074511339\n",
      "kings 0.7876614074511339\n",
      "KINGS 0.7876614074511339\n",
      "prince 0.7337737081547175\n",
      "PRINCE 0.7337737081547175\n",
      "Prince 0.7337737081547175\n"
     ]
    }
   ],
   "source": [
    "# Get 10 most-similar words to 'king'\n",
    "# argsort returns lowest to highest, so we get the last 10, then reverse\n",
    "top_n = np.argsort(similarities)[-10:][::-1] \n",
    "for i in top_n:\n",
    "    print(vocab_list[i], similarities[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the underlying vectors are obviously not case sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king     \t0.8024\n",
      "queen     \t0.7881\n",
      "prince     \t0.6401\n",
      "kings     \t0.6209\n",
      "princess     \t0.6126\n",
      "royal     \t0.5801\n",
      "throne     \t0.5787\n"
     ]
    }
   ],
   "source": [
    "# Most similar to our king - man + woman vector\n",
    "# Also, make this prettier and with less repetition\n",
    "\n",
    "def print_most_similar(matrix, vector, vocab_list, n=20):\n",
    "    sim = np.dot(matrix, vector/np.linalg.norm(vector))\n",
    "    sorted_sims = np.argsort(sim)[-n:][::-1] #argsort return lowest to highest, so we get the last n, then reverse\n",
    "    last_word = ''\n",
    "    for i in sorted_sims:\n",
    "        current_word = vocab_list[i].lower()\n",
    "        if current_word != last_word: # filter out duplicates\n",
    "            print(f\"{current_word}     \\t{sim[i]:.4}\")\n",
    "            last_word = current_word\n",
    "            \n",
    "print_most_similar(vector_matrix, king_to_queen_vector, vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure spacy\n",
    "\n",
    "Do the same thing, but entirely within spacy ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = nlp.vocab.vectors.most_similar(king_to_queen_vector.reshape(1,-1), n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[13176088972490086564, 14826469074451677028,  7464393751932445219,\n",
       "          7102492827649024548,  4176741725343376093,  5247273317732208552,\n",
       "         10168488388102651113, 11742085837932180620, 12278543830867659210,\n",
       "          4527521648030784477,  5578855234836721309,  3708855936003160130,\n",
       "          6599815902708227193, 13482751041672084183, 10227307316083268331,\n",
       "          8637828561746775750,   982181824006872346,  9482215189757771475,\n",
       "         10275277408386923532, 13488378799514498820]], dtype=uint64),\n",
       " array([[391587,   2182,   3149,  27269,   6025,  59855,   5309,  94888,\n",
       "          11899,   7473,   6602,  58141,   9575,  91940,   8297,   9117,\n",
       "           9740,  95981,   7926,  19961]], dtype=int32),\n",
       " array([[0.8024, 0.8024, 0.8024, 0.8024, 0.7881, 0.7881, 0.7881, 0.6401,\n",
       "         0.6401, 0.6401, 0.6209, 0.6209, 0.6209, 0.6126, 0.6126, 0.6126,\n",
       "         0.5801, 0.5801, 0.5801, 0.5787]], dtype=float32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the output\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output format of spacy's `most_similar()` function is a tuple that contains three items: an array of word hashes (unique IDs), an array of index positions in the vector matrix, and an array of similarity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king     \t0.8024\n",
      "queen     \t0.7881\n",
      "prince     \t0.6401\n",
      "kings     \t0.6209\n",
      "princess     \t0.6126\n",
      "royal     \t0.5801\n",
      "throne     \t0.5787\n"
     ]
    }
   ],
   "source": [
    "# Print top matches\n",
    "hashes, lines, scores = best\n",
    "last_word = ''\n",
    "for i in range(len(hashes[0])):\n",
    "    current_word = nlp.vocab[hashes[0][i].item()].text.lower()\n",
    "    if current_word != last_word:\n",
    "        print(f'{current_word}     \\t{scores[0][i]:.4}')\n",
    "        last_word = current_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_similar(word):\n",
    "    target_vector = nlp.vocab[word].vector/nlp.vocab[word].vector_norm\n",
    "    hashes, lines, scores = nlp.vocab.vectors.most_similar(target_vector.reshape(1,-1), n=20)\n",
    "    last_word = ''\n",
    "    for i in range(len(hashes[0])):\n",
    "        current_word = nlp.vocab[hashes[0][i].item()].text.lower()\n",
    "        if current_word != last_word:\n",
    "            print(f'{current_word}     \\t{scores[0][i]:.4}')\n",
    "            last_word = current_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "economy     \t1.0\n",
      "economic     \t0.834\n",
      "economies     \t0.7998\n",
      "recession     \t0.7351\n",
      "downturn     \t0.696\n",
      "crisis     \t0.6726\n",
      "inflation     \t0.6628\n",
      "\n",
      "economic     \t1.0\n",
      "economy     \t0.834\n",
      "economies     \t0.7699\n",
      "crisis     \t0.7357\n",
      "financial     \t0.728\n",
      "monetary     \t0.6949\n",
      "political     \t0.6929\n",
      "\n",
      "money     \t1.0\n",
      "cash     \t0.8191\n",
      "dollars     \t0.7552\n",
      "pay     \t0.7542\n",
      "paying     \t0.742\n",
      "funds     \t0.7177\n",
      "paid     \t0.7\n",
      "\n",
      "wealth     \t1.0\n",
      "prosperity     \t0.7057\n",
      "riches     \t0.6847\n",
      "fortune     \t0.6369\n",
      "wealthy     \t0.623\n",
      "investment     \t0.6076\n",
      "rich     \t0.6042\n",
      "\n",
      "child     \t1.0\n",
      "children     \t0.8332\n",
      "parents     \t0.7356\n",
      "infant     \t0.724\n",
      "kids     \t0.7076\n",
      "toddler     \t0.7038\n",
      "\n",
      "housework     \t1.0\n",
      "chores     \t0.8052\n",
      "yardwork     \t0.6721\n",
      "chore     \t0.6643\n",
      "errands     \t0.6356\n",
      "schoolwork     \t0.6183\n",
      "housecleaning     \t0.6037\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get most similar vectors to each of a list of target terms\n",
    "for term in ['economy', 'economic', 'money', 'wealth', 'child', 'housework']:\n",
    "    print_most_similar(term)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document similarities using word embeddings\n",
    "\n",
    "It's fun to study words, but we mostly care about documents. How can we use word embeddings to perform the kinds of document-similarity tasks that have occupied us so far this semester?\n",
    "\n",
    "The answer is that we replace each word token with its corresponding embedding, then, for each document, average the embeddings to get a representation of the document. We can then use those document representations (of dimensionality equal to the dimensionality of our embedding vectors) as input to any of the methods we've already discussed.\n",
    "\n",
    "Here, we use our derived document representations to measure the cosine similarity between different (toy) documents. We do this both with and without removing stopwords, and we compare our measures to those using word counts (rather than embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 0: The young cat is smaller than the old dog.\n",
      "Doc 1: The baby kitten is tinier than the elderly hound.\n",
      "   Built-in similarity: 0.923\n",
      "   Hand similarity:     0.923\n",
      "   Embed sim, no stops: 0.782\n",
      "   Token sim, no stops: 0.0\n",
      "\n",
      "Doc 0: The young cat is smaller than the old dog.\n",
      "Doc 2: Not all sentences are about animals, nor about their properties.\n",
      "   Built-in similarity: 0.789\n",
      "   Hand similarity:     0.789\n",
      "   Embed sim, no stops: 0.492\n",
      "   Token sim, no stops: 0.0\n",
      "\n",
      "Doc 0: The young cat is smaller than the old dog.\n",
      "Doc 3: It is a truth universally acknowledged ...\n",
      "   Built-in similarity: 0.735\n",
      "   Hand similarity:     0.735\n",
      "   Embed sim, no stops: 0.312\n",
      "   Token sim, no stops: 0.0\n",
      "\n",
      "Doc 1: The baby kitten is tinier than the elderly hound.\n",
      "Doc 2: Not all sentences are about animals, nor about their properties.\n",
      "   Built-in similarity: 0.719\n",
      "   Hand similarity:     0.719\n",
      "   Embed sim, no stops: 0.364\n",
      "   Token sim, no stops: 0.0\n",
      "\n",
      "Doc 1: The baby kitten is tinier than the elderly hound.\n",
      "Doc 3: It is a truth universally acknowledged ...\n",
      "   Built-in similarity: 0.688\n",
      "   Hand similarity:     0.688\n",
      "   Embed sim, no stops: 0.227\n",
      "   Token sim, no stops: 0.0\n",
      "\n",
      "Doc 2: Not all sentences are about animals, nor about their properties.\n",
      "Doc 3: It is a truth universally acknowledged ...\n",
      "   Built-in similarity: 0.762\n",
      "   Hand similarity:     0.762\n",
      "   Embed sim, no stops: 0.389\n",
      "   Token sim, no stops: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Doc similarities\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Set up sample sentences\n",
    "sentences = [\n",
    "    'The young cat is smaller than the old dog.',\n",
    "    'The baby kitten is tinier than the elderly hound.',\n",
    "    'Not all sentences are about animals, nor about their properties.',\n",
    "    'It is a truth universally acknowledged ...'\n",
    "]\n",
    "\n",
    "# process sentences into spacy nlp objects\n",
    "docs = []\n",
    "for sentence in sentences:\n",
    "    tokens = nlp(sentence)\n",
    "    docs.append(tokens)\n",
    "    \n",
    "def remove_noninformative_tokens(doc):\n",
    "    '''\n",
    "    Takes a spacy-processed document.\n",
    "    Returns a list of spacy token objects without stopwords, punctuation, or embedding-less tokens.\n",
    "    '''\n",
    "    culled = [token for token in doc if not (token.is_stop or token.is_punct) and token.has_vector]\n",
    "    return(culled)\n",
    "\n",
    "def embedding_similarity(X,Y):\n",
    "    '''\n",
    "    Takes two lists of spacy token objects.\n",
    "    Returns cosine similarity between their embedding representations.\n",
    "    '''\n",
    "    mean_vector_X = np.mean([token.vector for token in X], axis=0)\n",
    "    mean_vector_Y = np.mean([token.vector for token in Y], axis=0)\n",
    "        # note below reshape 1D -> 2D array. 2D version is 1 row x n columns\n",
    "    cos_similarity = cosine_similarity(mean_vector_X.reshape(1, -1), mean_vector_Y.reshape(1, -1))\n",
    "    return(cos_similarity.item()) # use .item() to get single value from array\n",
    "    \n",
    "def token_similarity(X,Y):\n",
    "    '''\n",
    "    Takes two lists of spacy tokens.\n",
    "    Returns cosine similarity between their tokens.\n",
    "    '''\n",
    "    doc1 = ' '.join([token.text for token in X])\n",
    "    doc2 = ' '.join([token.text for token in Y])\n",
    "    vectorizer = TfidfVectorizer(use_idf=False)\n",
    "    vectors = vectorizer.fit_transform([doc1, doc2])\n",
    "    return(cosine_similarity(vectors[0], vectors[1]).item())\n",
    "    \n",
    "for i, doc in enumerate(docs):\n",
    "    doc1_nostops = remove_noninformative_tokens(doc)\n",
    "    for j in range(len(docs)):\n",
    "        if j>i:\n",
    "            doc2_nostops = remove_noninformative_tokens(docs[j])\n",
    "            print(f\"Doc {i}: {docs[i]}\")\n",
    "            print(f\"Doc {j}: {docs[j]}\")\n",
    "            print(f\"   Built-in similarity: {doc.similarity(docs[j]):.3}\")\n",
    "            print(f\"   Hand similarity:     {embedding_similarity(doc, docs[j]):.3}\")\n",
    "            print(f\"   Embed sim, no stops: {embedding_similarity(doc1_nostops, doc2_nostops):.3}\")\n",
    "            print(f\"   Token sim, no stops: {token_similarity(doc1_nostops, doc2_nostops):.3}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
