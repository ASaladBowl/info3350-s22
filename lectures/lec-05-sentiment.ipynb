{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 05: Sentiment scoring\n",
    "\n",
    "## To do\n",
    "\n",
    "* Readings\n",
    "    * *HDA* ch. 4 for today\n",
    "    * Two articles for Weds\n",
    "* **Response posts.** See Canvas for schedule and instructions. \n",
    "    * **Optional** response for *this week*; due by Tuesday at 4pm.\n",
    "        * Most people will not write  a response this week\n",
    "        * Use if you know you'll miss a later one, or are very interested in this week's topic\n",
    "    * The readings themselves are *always* required. Read every week. Respond only as scheduled.\n",
    "* HW2 due Thursday night at 11:59pm\n",
    "* Remember that Ed is the place for questions about any aspect of the course. \n",
    "    * Follow up in office hours as needed.\n",
    "* Classroom for lectures is likely to change. Will announce via Ed if/when it does.\n",
    "\n",
    "## What are we doing?\n",
    "\n",
    "We want to determine the *sentiment* of a text and of the individual sentences from which the text is composed.\n",
    "\n",
    "\"Sentiment\" can mean a lot of things:\n",
    "\n",
    "* Positive and negative feelings\n",
    "* Emotional intensity (could be good or bad)\n",
    "* Amount or intensity of other emotions (joy, surprise, awe, fear, etc.)\n",
    "* Maybe even \"sentimentalness\" (roughly, \"nostalgia\")\n",
    "\n",
    "Today, we'll focus on sentiment as the expression of positive and negative feelings at the token level. This is a common case in many text anlaysis problems.\n",
    "\n",
    "## Supervised and unsupervised\n",
    "\n",
    "There are two broad ways we could approach the task of senitment analysis:\n",
    "\n",
    "* **Unsupervised** methods start with **known-informative features** and produce labels or scores from those features.\n",
    "* **Supervised** methods start with **labeled data** and try to learn the features that best predict the labels.\n",
    "* Advantages and disadvantages of each\n",
    "  * In short: unfront labeling costs vs. later validation costs\n",
    "  \n",
    "## Warning\n",
    "\n",
    "Every semester, we see a surprising number of student projects that are built around unsupervised sentiment analysis. These projects tend to be weak, since they rely on a method covered in week 3 that has lots of known limitations and many superior alternatives. Do not fall into this trap.\n",
    "\n",
    "## Our method\n",
    "\n",
    "We will work, for now, with **unsupervised** sentiment analysis. But there are lots of supervised approaches, too.\n",
    "\n",
    "Specifically, we're going to use so-called lexical or dictionary-based methods that assign one or more emotions to a subset of English words. We will assume that each of those words in a given text is an indication that the text contains the corresponding emotion. We can them sum up the emotions over all words in the text to get a measurement of that text's net emotional content.\n",
    "\n",
    "**Quick exercise:** Do you expect this method to work? What are some potential problems?\n",
    "\n",
    "## Example case\n",
    "\n",
    "* From Jockers' [*Syuzhet* vignette](https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html). \n",
    " * Note that this is an R package. We can't use it directly.\n",
    "* \"Syuzhet\" = \"plot\" or \"subject\" in Russian; it refers to narrative order, rather than to the \"true\" order of the narrative's underlying events (which is called the *fabula*).\n",
    "* To see the difference, think about a flashback that occurs near the end of a story.\n",
    "\n",
    "Consider the following story (or \"story\"):\n",
    "\n",
    "> I begin this story with a neutral statement.  \n",
    "  Basically this is a very silly test.  \n",
    "  You are testing the Syuzhet package using short, inane sentences.  \n",
    "  I am actually very happy today. \n",
    "  I have finally finished writing this package.  \n",
    "  Tomorrow I will be very sad. \n",
    "  I won't have anything left to do. \n",
    "  I might get angry and decide to do something horrible.  \n",
    "  I might destroy the entire package and start from scratch.  \n",
    "  Then again, I might find it satisfying to have completed my first R package. \n",
    "  Honestly this use of the Fourier transformation is really quite elegant.  \n",
    "  You might even say it's beautiful!\n",
    "\n",
    "### Score some sentences ...\n",
    "\n",
    "By show of hands, how many think each sentence is:\n",
    "* Positive\n",
    "* Neutral\n",
    "* Negative\n",
    "\n",
    "The sentences:\n",
    "1. **Basically this is a very silly test.**\n",
    "1. **You are testing the Syuzhet package using short, inane sentences.**  \n",
    "1. **I have finally finished writing this package.**  \n",
    "1. **I won't have anything left to do.** \n",
    "1. **Honestly this use of the Fourier transformation is really quite elegant.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter eyeballed values from in-class survey ...\n",
    "# Range [-2, 2] strong negative to strong positive\n",
    "silly    = 0\n",
    "inane    = 0\n",
    "finished = 0\n",
    "left     = 0\n",
    "fourier  = 0\n",
    "\n",
    "# scores for all sentences, includings ones not recorded above\n",
    "human_scores = [\n",
    "    0,\n",
    "    silly,\n",
    "    inane,\n",
    "    2,\n",
    "    finished,\n",
    "    -2,\n",
    "    left,\n",
    "    -2,\n",
    "    -2,\n",
    "    1,\n",
    "    fourier,\n",
    "    1.5\n",
    "]\n",
    "\n",
    "print(\"Human scores by sentence:\", human_scores)\n",
    "print(\"Overall human sentiment score:\", round(sum(human_scores),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confidence check:** Does this strike us as a reasonable summary of the overall positive-negative affect of the sample story? If not, why not?\n",
    "\n",
    "### Ingest and tokenize the example \"story\"\n",
    "\n",
    "Notice that our output data structure is a list of lists. The \"outer\" list contains sentences. The \"inner\" lists each contain the tokens in one sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "# The story. Why triple quotes?\n",
    "story = '''\\\n",
    "  I begin this story with a neutral statement.  \n",
    "  Basically this is a very silly test.  \n",
    "  You are testing the Syuzhet package using short, inane sentences.  \n",
    "  I am actually very happy today. \n",
    "  I have finally finished writing this package.  \n",
    "  Tomorrow I will be very sad. \n",
    "  I won't have anything left to do. \n",
    "  I might get angry and decide to do something horrible.  \n",
    "  I might destroy the entire package and start from scratch.  \n",
    "  Then again, I might find it satisfying to have completed my first R package. \n",
    "  Honestly this use of the Fourier transformation is really quite elegant.  \n",
    "  You might even say it's beautiful!'''\n",
    "\n",
    "# Tokenize the story\n",
    "tokens = [word_tokenize(sent.lower()) for sent in sent_tokenize(story)]\n",
    "print(\"Sentences:\", len(tokens))\n",
    "print(\"Total tokens:\", sum([len(sent) for sent in tokens]))\n",
    "print(\"\\nSample sentences:\", tokens[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick aside: **list comprehension**:\n",
    "\n",
    "* It's a compact way to write a for loop and store the result in a list.\n",
    "* Good for quick stuff, but not flexible and not very legible.\n",
    "\n",
    "```\n",
    "tokens = [word_tokenize(sent.lower()) for sent in sent_tokenize(story)]\n",
    "```\n",
    "\n",
    "is the same as\n",
    "\n",
    "```\n",
    "tokens = []\n",
    "for sent in sent_tokenize(story):\n",
    "    tokens.append(word_tokenize(sent.lower()))\n",
    "```\n",
    "\n",
    "### Set up sentiment dictionaries\n",
    "\n",
    "We want to compare a couple of them. Specifically, we'll use:\n",
    "\n",
    "* NLTK's copy of Hu and Liu's lexicon ([paper](https://www.cs.uic.edu/~liub/publications/kdd04-revSummary.pdf) | [dataset](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html))\n",
    "* Mohammad's [EmoLex](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) from the Canadian National Research Council (NRC). \n",
    "    * Crowd-sourced word associations.\n",
    "    * Some are ... questionable? See HW3.\n",
    "\n",
    "Are these good and suitable choices for our task? Let's (begin to) find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   collections import defaultdict\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "\n",
    "nltk.download('opinion_lexicon') # Need to download this the first time used\n",
    "\n",
    "# NLTK simple lexicon (from Hu and Liu (2004))\n",
    "nltk_lexicon = {\n",
    "    'positive' : set(nltk.corpus.opinion_lexicon.positive()), # Why cast to a set?\n",
    "    'negative' : set(nltk.corpus.opinion_lexicon.negative())\n",
    "}\n",
    "\n",
    "# Print a sample of the NLTK lexicon\n",
    "print('NLTK lexicon sample')\n",
    "for key in nltk_lexicon.keys():\n",
    "    print(f'{key}:', random.sample(tuple(nltk_lexicon[key]), 5))\n",
    "    \n",
    "# NRC EmoLex lexicon (from Mohammad, http://sentiment.nrc.ca/lexicons-for-research/)\n",
    "# No package for this, just read the data from a local file\n",
    "emolex_file = os.path.join('..', 'data', 'lexicons', 'emolex.txt')\n",
    "nrc_lexicon = defaultdict(dict) # Like Counter(), defaultdict eases dictionary creation\n",
    "with open(emolex_file, 'r') as f:\n",
    "    # emolex file format is: word emotion value\n",
    "    for line in f:\n",
    "        word, emotion, value = line.strip().split()\n",
    "        nrc_lexicon[word][emotion] = int(value)\n",
    "        \n",
    "# Print a sample of the NRC EmoLex lexicon\n",
    "print('\\nNRC lexicon sample')       \n",
    "for key in random.sample(tuple(nrc_lexicon.keys()), 2):\n",
    "    print(f'{key}:', nrc_lexicon[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scoring function\n",
    "\n",
    "Set up a function to score a word as positive or negative using either the NLTK or NRC dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_sentiment_score(word, method='nrc', lex=nrc_lexicon):\n",
    "    '''\n",
    "    Takes a word, optional method in ['nrc', 'nltk'], and optional lexicon dictionary.\n",
    "    Returns 1 (if positive), -1 (if negative), 0 (neutral), or None (not in lex).\n",
    "    '''\n",
    "    word = word.lower() # Handle non-case-folded inputs\n",
    "    if method.lower() == 'nrc':\n",
    "        if word in lex: # Only score words that are in the lexicon\n",
    "            pos = lex[word]['positive']\n",
    "            neg = lex[word]['negative']\n",
    "            if pos == neg: # Ties (mostly 0==0) return zero\n",
    "                return 0\n",
    "            elif pos > neg:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "    elif method.lower() == 'nltk':\n",
    "        if word in lex['positive']:\n",
    "            return 1\n",
    "        elif word in lex['negative']:\n",
    "            return -1\n",
    "    else:\n",
    "        raise NameError(\"Method not in ['nrc', 'nltk']\")\n",
    "    return None # If word not in lexicon, return None (not zero). Why do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NRC  'analyst':\", word_sentiment_score('analyst'))\n",
    "print(\"NLTK 'analyst':\", word_sentiment_score('analyst', method='nltk', lex=nltk_lexicon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score our example sentences\n",
    "\n",
    "For reference, here are our class-crowd-sourced scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human scores\n",
    "print(\"Method: human\")\n",
    "print(\"Sentence scores:\", human_scores)\n",
    "print(\"Summary score:\", sum(human_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and compare dictionary scores\n",
    "\n",
    "# Scoring methods for input\n",
    "methods = {\n",
    "    'nltk':nltk_lexicon,\n",
    "    'nrc' :nrc_lexicon\n",
    "}\n",
    "\n",
    "# Calculate, record, and print scores\n",
    "method_scores = {} # To store results\n",
    "for method in methods:\n",
    "    sentence_scores = [] # Could rewrite next few lines as a list comprehension\n",
    "    for sent in tokens:\n",
    "        sentence_score = 0\n",
    "        for word in sent:\n",
    "            word_score = word_sentiment_score(word, method=method, lex=methods[method])\n",
    "            if word_score != None:\n",
    "                sentence_score += word_score\n",
    "        sentence_scores.append(sentence_score)\n",
    "    method_scores[method] = sentence_scores # Save sentence-level scores\n",
    "    # Print results\n",
    "    print(\"Method:\", method)\n",
    "    print(\"Sentence scores:\", sentence_scores)\n",
    "    print(\"Summary score:\", sum(sentence_scores),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize and print the story for discussion\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = list(range(len(tokens)))\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "plt.plot(x, human_scores, '-',  c='black', label='human', alpha=0.7, linewidth=3)\n",
    "plt.plot(x, method_scores['nltk'], '--', c='blue', label='nltk', alpha=0.7, linewidth=3)\n",
    "plt.plot(x, method_scores['nrc'], '-.', c='red', label='nrc', alpha=0.7, linewidth=3)\n",
    "plt.legend()\n",
    "plt.title(\"Sentiment scores\")\n",
    "plt.show()\n",
    "\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss \n",
    "\n",
    "Do these scores make sense? \n",
    "\n",
    "* We can look at specific instances, like NRC on the first sentence (next code block)\n",
    "* **Is this a happy story or not?**\n",
    " * Do the summary scores reflect our judgment about that?\n",
    " * If not, why not and how could we improve those scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token-level NRC scores for example sentence 1\n",
    "for word in tokens[0]:\n",
    "    print(f'{word.ljust(11)}{word_sentiment_score(word)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Seaborn to plot data with lowess (local regression) fit\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context('talk')\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "sns.regplot(x=x, y=human_scores, lowess=True, color='k', label='human')\n",
    "sns.regplot(x=x, y=method_scores['nrc'], lowess=True, color='r', label='nrc')\n",
    "sns.regplot(x=x, y=method_scores['nltk'], lowess=True, color='b', label='nltk')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Madame Bovary* (Flaubert, 1856/1857)\n",
    "\n",
    "Expected arc: starts happy, ends sad.\n",
    "\n",
    "Note that we do not lowercase our text (why not?), nor do we remove stopwords and punctuation (again, why not?). We *could* do both of those things, but would want to be sure that there weren't any name collisions in our text (that is, names that have affective associations when used as common words). We might also want to preserve case and punctuation for other tasks in our processing pipeline, even if we don't need them for our sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and tokenize the novel\n",
    "bovary_file = os.path.join('..', 'data', 'texts', 'F-Flaubert-Madame_Bovary-1857-M.txt')\n",
    "with open(bovary_file, 'r') as f:\n",
    "    bovary_text = f.read()\n",
    "bovary = [word_tokenize(sent) for sent in sent_tokenize(bovary_text)]\n",
    "print(\"Sentences:\", len(bovary))\n",
    "print(\"Total tokens:\", sum([len(sent) for sent in bovary]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note one small change below: we divide sentence sentiment by sentence length, so that long sentences don't count more than short ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score it using NLTK and NRC methods\n",
    "bovary_scores = {}\n",
    "for method in methods:\n",
    "    sentence_scores = []\n",
    "    for sent in bovary:\n",
    "        sentence_score = 0\n",
    "        for word in sent:\n",
    "            word_score = word_sentiment_score(word, method=method, lex=methods[method])\n",
    "            if word_score != None:\n",
    "                sentence_score += word_score\n",
    "        sentence_scores.append(sentence_score/len(sent))\n",
    "    bovary_scores[method] = sentence_scores\n",
    "    print(\"Method:\", method)\n",
    "    print(\"Summary score:\", round(sum(bovary_scores[method]),2),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results with 4th-order polynomial fit (why?)\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "x_bov = [i/len(bovary) for i in range(len(bovary))]\n",
    "sns.regplot(x=x_bov, y=bovary_scores['nltk'], order=4, scatter=False, color='b', label=None)\n",
    "sns.regplot(x=x_bov, y=bovary_scores['nltk'], x_bins=20, scatter=True, fit_reg=False, color='b', label='nltk')\n",
    "sns.regplot(x=x_bov, y=bovary_scores['nrc'], order=4, scatter=False, color='r', label=None)\n",
    "sns.regplot(x=x_bov, y=bovary_scores['nrc'], x_bins=20, scatter=True, fit_reg=False, color='r', label='nrc')\n",
    "plt.title(\"Sentiment in $\\it{Madame\\ Bovary}$\")\n",
    "plt.xlabel(\"Narrative time\")\n",
    "plt.ylabel(\"Average binned sentiment\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss:** How would you compare and evaluate the results of these two methods on this text?\n",
    "\n",
    "## Where to go from here\n",
    "\n",
    "* More validation!\n",
    "  * Is your lexicon good? For this kind of text? Written at this time?\n",
    "* Other dictionaries\n",
    "  * Develop your own?\n",
    "  * See Jurafsky and Martin for clever and/or complex ideas\n",
    "    * Example: tag adjectives. Those that appear on either side of the token `and` probably have the same sentiment polarity; those linked by `but` are likely opposites.\n",
    "    * Lots of embedding-based approaches, too.\n",
    "* Other aspects of sentiment/emotion/affect \n",
    "  * NRC 'anger', 'surprise', 'trust', etc.\n",
    "* Other texts and other *types* of text\n",
    "* **Combine sentiment with other kinds of text- and sentence-level scoring**\n",
    "  * Gender, time period (linguistic drift, yikes!), translations of the same text, news coverage of candidates, ...\n",
    "  * Problem set 3 will try this\n",
    "* More ideas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
