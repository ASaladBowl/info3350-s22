{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 09: Clustering (and TF-IDF weighting)\n",
    "\n",
    "## To do\n",
    "\n",
    "* Reading for Weds: Allison et al., Mauch et al.\n",
    "    * Response due by Tuesday at 4:00pm for those designated this week\n",
    "* HW4 (distance metrics) due by Thursday night at 11:59.\n",
    "* How to get help\n",
    "    * Start early\n",
    "    * Read the documentation\n",
    "    * Join a study group\n",
    "    * Read any error messages\n",
    "        * Key info is often at the **bottom** of the error output\n",
    "    * Explain your problem to someone else\n",
    "    * Document what you've done\n",
    "    * Post publicly on Ed\n",
    "    * Come to office hours with specific questions\n",
    "        * TAs will not write code for you; they will ask you what you're trying to do, what you've done, and what the problem is\n",
    "        * TAs will give you things to consider\n",
    "\n",
    "## Aside: TF-IDF weighting for word counts\n",
    "\n",
    "* Why do we sometimes remove stopwords from our features?\n",
    "    * High-frequency words shared by many documents don't tell us (in many, but not all, cases) much about the similarities or differences between documents\n",
    "* But stopword lists are binary: a word is either a stopword (hence, removed) or it isn't\n",
    "* Can we define a continuous adjustment for \"stoppiness\" that we apply to *every* word, depending on how widely used it is?\n",
    "* One approach is \"term frequency-inverse document frequency\" (TFIDF) weighting. \n",
    "    * You can think of this as multiplying the count of each term in a document by the inverse of the fraction of all documents in which that word occurs (hence \"term frequency [multiplied by] inverse document frequency\"). It's a bit more complicated than that (see below), but that's the idea. This upweights words that occur in relatively few documents.\n",
    "    * The count of a word that occurs in every document would be multiplied by one, hence get no boost in each document. A word that occurs in just one document in a corpus of 100 documents would be multiplied by 100 in the one document that contains it.\n",
    "* There are several tweaks to TFIDF to smooth it out and to modulate the boost it provides. `scikit-learn`'s `TfidfVectorizer` applies the reweighting:\n",
    "\n",
    "$$idf(t) = \\ln\\frac{1+n}{1 + df(t)} + 1$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $t$ is the term in question\n",
    "* $idf(t)$ is the inverse document weight to be applied to the count of term $t$\n",
    "* $n$ is the number of documents in the corpus\n",
    "* $df(t)$ is the number of documents in the corpus that contain term $t$\n",
    "\n",
    "A toy example: Consider two documents:\n",
    "\n",
    "* Document 1: `\"cat dog\"`\n",
    "* Document 2: `\"dog dog\"`\n",
    "\n",
    "`cat` occurs in just one document; `dog` occurs in both documents. So we want (and expect) to upweight the count of `cat` in document 1.\n",
    "\n",
    "Calculate the `idf` weight for `cat` in document 1:\n",
    "\n",
    "* $n = 2$\n",
    "* $df(\\text{'cat'}) = 1$\n",
    "\n",
    "$$idf(\\text{'cat'}) = \\ln\\frac{1 + 2}{1 + 1} + 1 = \\ln\\frac{3}{2} + 1 = 1.405$$\n",
    "\n",
    "And for `dog` in document 1:\n",
    "\n",
    "* $n = 2$\n",
    "* $df(\\text{'dog'}) = 2$\n",
    "\n",
    "$$idf(\\text{'dog'}) = \\ln\\frac{1 + 2}{1 + 2} + 1 = \\ln\\frac{3}{3} + 1 = 1.0$$\n",
    "\n",
    "So, `cat` will be upweighted relative to `dog`, because it is the less widely used word across documents in the corpus.\n",
    "\n",
    "Our non-normalized but IDF-weighted feature matrix will look like this:\n",
    "\n",
    "```\n",
    "cat dog\n",
    "1.4 1.0\n",
    "0   2.0\n",
    "```\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"cat dog\",\n",
    "    \"dog dog\",\n",
    "]\n",
    "\n",
    "# without IDF weighting (note l2 norm)\n",
    "vectorizer_no_idf = TfidfVectorizer(\n",
    "    use_idf=False\n",
    ")\n",
    "features_no_idf = vectorizer_no_idf.fit_transform(corpus)\n",
    "print(\"### Feature matrix *without* IDF weighting ###\")\n",
    "print(\"Feature names:\", vectorizer_no_idf.get_feature_names_out())\n",
    "print(features_no_idf.toarray())\n",
    "\n",
    "# with IDF weighting\n",
    "vectorizer_with_idf = TfidfVectorizer(\n",
    "    use_idf=True\n",
    ")\n",
    "features_with_idf = vectorizer_with_idf.fit_transform(corpus)\n",
    "print(\"\\n### Feature matrix *with* IDF weighting ###\")\n",
    "print(\"Feature names:\", vectorizer_with_idf.get_feature_names_out())\n",
    "print(features_with_idf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, in document 1, `cat` has been up-weighted while `dog` has been downweighted. There's no change in document 2 because that document has only a single word type and `TfidfVectorizer`'s `l2` norm enforces total feature weights whose squares sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our hand calculation against code version\n",
    "import numpy as np\n",
    "vec = np.array([1.405, 1.0])     # hand calculation\n",
    "l2_vec = vec/np.linalg.norm(vec) # calculate l2 normalized version\n",
    "print(\"l2-normed, hand calculated, IDF weighted features for document 1\")\n",
    "print(l2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduce assert statment\n",
    "assert np.allclose(l2_vec, features_with_idf[0,].toarray(), atol=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised and unsupervised learning\n",
    "\n",
    "* \"Clustering\" is a type of *unsupervised* learning\n",
    "* \"Classification\" (next week) is a type of *supervised* learning\n",
    "* Both seek to assign a finite set of labels to input objects on the basis of features associated with those objects\n",
    "* In *supervised* learning, you know the labels in advance\n",
    "    * You supply a set of (correctly) labeled objects to the algorithm\n",
    "    * The algorithm then \"learns\" which features are associated with which labels, so as to minimize classification errors\n",
    "* In *unsupervised* learning, you don't know the labels (though you might know -- or at least have a sense about -- how *many* labels there should be)\n",
    "    * You supply a feature matrix and a definition of what a \"good\" clustering should be\n",
    "    * The algorithm then assigns labels to the input objects so as to best satisfy the supplied definition of \"good\" (typically, minimizing variance within clusters and maximizing difference between clusters)\n",
    "\n",
    "## Why prefer one or the other?\n",
    "\n",
    "* Unsupervised methods are often used early in a project, when you're looking for unknown stucture in your data\n",
    "    * Unsupervised methods are your only option if you don't know what the appropriate set of labels might be for your data set\n",
    "    * Also appropriate if you don't have (many) labeled instances\n",
    "    * Unsupervised methods are typically \"cheap\" to set up, but costly to evaluate\n",
    "* Supervised methods require you to know in advance the full set of appropriate labels for your data\n",
    "    * Supervised methods often have high initial costs, but are easier to evaluate (because you already have a set of correctly labeled instances that you can use for validation).\n",
    "\n",
    "## Cluster boundaries\n",
    "\n",
    "[sketch the problem]\n",
    "\n",
    "* Note that not all clusters have the same general shape\n",
    "* Spherical blobs, separate linear blobs, areas of varying density, etc.\n",
    "* There is no universally best way to draw decision boundaries\n",
    "\n",
    "## *k*-means clustering\n",
    "\n",
    "The algorithm:\n",
    "\n",
    "1. Select *k* objects from the data set to serve as initial cluster centers (\"centroids\")\n",
    "1. Assign each object in the data set to the nearest centroid. These are your initial clusters.\n",
    "1. Calculate the mean location of the objects assigned to each cluster. These are your new centroids.\n",
    "1. If the new centroids are sufficiently close to the previous centroids, you're done.\n",
    "    1. If the new centroids are not sufficiently close to the old centroids, use the new centroids as the basis for a new clusering.\n",
    "    1. Repeat labeling, centroid calculation, and difference comparison until centroids are stable (enough).\n",
    "\n",
    "## Other clustering methods\n",
    "\n",
    "* Ward (hierarchical), agglomerative\n",
    "* Density-based (DBSCAN)\n",
    "* Graph distance\n",
    "* ...\n",
    "\n",
    "## An artificial example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Means on articial data\n",
    "# Adapted from sklearn examples\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# make artificial data (demo only, not part of the real-world process)\n",
    "n_samples = 1500\n",
    "random_state = 42\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "\n",
    "# Unlabeled data\n",
    "plt.subplot(231)\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.title(\"Unclustered data\")\n",
    "\n",
    "# Correct number of clusters\n",
    "#  Note we are doing three steps (instantiate, fit, predict)\n",
    "#  in one line of code here\n",
    "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X)\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.title(\"k=3\")\n",
    "\n",
    "\n",
    "# Incorrect number of clusters\n",
    "y_pred = KMeans(n_clusters=2, random_state=random_state).fit_predict(X)\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.title(\"k=2\")\n",
    "\n",
    "# Different variance\n",
    "X_varied, y_varied = make_blobs(n_samples=n_samples,\n",
    "                                cluster_std=[1.0, 5.0, 1.0],\n",
    "                                random_state=random_state)\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.scatter(X_varied[:, 0], X_varied[:, 1])\n",
    "plt.title(\"Unequal density\")\n",
    "\n",
    "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_varied)\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\n",
    "plt.title(\"k=3\")\n",
    "\n",
    "y_pred = KMeans(n_clusters=2, random_state=random_state).fit_predict(X_varied)\n",
    "\n",
    "plt.subplot(236)\n",
    "plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\n",
    "plt.title(\"k=2\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texts as high-dimensional data\n",
    "\n",
    "As noted in the last problem set, we usually deal with far more than 2 dimensions of texts. If we use word counts as our features, it's easy to have 10,000 or 100,000 or more dimensions in our data set. We'll talk about this in a future lecture, but note for now that the problem of high-dimensional feature space is that **everything becomes far away from everything else**. This makes distance metrics and density measures less meaningful than they are in low-dimensional spaces.\n",
    "\n",
    "One solution is **dimension reduction**, which ranges from the relatively simple (singular value decomposition, principal component analysis) to the very complex, indeed (vector embeddings from deep neural networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
